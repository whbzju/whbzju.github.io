<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.104.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Wujia's Blog</title><meta name=description content><meta name=author content><link rel=canonical href=https://whbzju.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><link rel=icon href=https://whbzju.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://whbzju.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://whbzju.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://whbzju.github.io/apple-touch-icon.png><link rel=mask-icon href=https://whbzju.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://whbzju.github.io/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Wujia's Blog"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://whbzju.github.io/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Wujia's Blog"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Wujia's Blog","url":"https://whbzju.github.io/","description":"","thumbnailUrl":"https://whbzju.github.io/favicon.ico","sameAs":["https://www.linkedin.com/in/haibo-wu-97b60759/","https://www.zhihu.com/people/wu-hai-bo"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://whbzju.github.io/ accesskey=h title="Wujia's Blog (Alt + H)">Wujia's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>Welcome</h1></header><div class=entry-content>大家好，欢迎来到吾加的Blog。记录自己的学习和实践总结，包括编程、机器学习算法、搜广推实践、程序员职业发展等。目前在一家国内的互联网上市公司担任技术和产品的负责人。</div><footer class=entry-footer><div class=social-icons><a href=https://www.linkedin.com/in/haibo-wu-97b60759/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://www.zhihu.com/people/wu-hai-bo target=_blank rel="noopener noreferrer me" title=Zhihu><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2>不一样的论文解读：2018 Real Time Personalization Using Embeddings for Search Ranking at Airbnb</h2></header><div class=entry-content><p>Airbnb这篇论文拿了今年KDD best paper，和16年google的W&D类似，并不fancy，但非常practicable，值得一读。可喜的是，据我所知，国内一线团队的实践水平并不比论文中描述的差，而且就是W&D，国内也有团队在论文没有出来之前就做出了类似的结果，可见在推荐这样的场景，大家在一个水平线上。希望未来国内的公司，也发一些真正实用的paper，不一定非要去发听起来fancy的。
自从Word2vec出来后，迅速应用到各个领域中，夸张一点描述，万物皆可embedding。在NLP中，一个困难是如何描述词，传统有onehot、ngram等各种方式，但它们很难表达词与词之间的语义关系，简单来讲，即词之间的距离远近关系。我们把每个词的Embedding向量理解成它在这个词表空间的位置，即位置远近能描述哪些词相关，那些词不相关。
对于互联网场景，比如电商、新闻，同样的，我们很难找到一个合适表达让计算机理解这些实体的含义。传统的方式一般是给实体打标签，比如新闻中的娱乐、体育、八卦等等。且不说构建一个高质量标签体系的成本，就其实际效果来讲，只能算是乏善可陈。类似NLP，完全可以将商品本身或新闻本身当做一个需要embedding的实体。当我们应用embedding方案时，一般要面对下面几个问题：
希望Embedding表达什么，即选择哪一种方式构建语料 如何让Embedding向量学到东西 如何评估向量的效果 线上如何使用 下面我们结合论文的观点来回答上面问题，水平有限，如有错误，欢迎指出。
希望Embedding表达什么 前面我们提了Embedding向量最终能表达实体在某个空间里面的距离关系，但并没有讲这个空间是什么。在NLP领域，这个问题不需要回答，就是语义空间，由我们文明中的各式各样的文本语料组成。在其他场景中，以电商举例，我们会直接对商品ID做Embedding，其训练的语料来至于用户的行为日志，故这个空间是用户的兴趣点组成。行为日志的类型不同，表达的兴趣也不同，比如点击行为、购买行为，表达的用户兴趣不同。故商品Embedding向量最终的作用，是不同商品在用户兴趣空间中的位置表达。
很多同学花很多时间在尝试各种word2vec的变种上，其实不如花时间在语料构建的细节上。首先，语料要多，论文中提到他们用了800 million search clicks sessions，在我们尝试Embedding的实践中，语料至少要过了亿级别才会发挥作用。其次，session的定义很重要。word2vec在计算词向量时和它context关系非常大，用户行为日志不像文本语料，存在标点符合、段落等标识去区分词的上下文。
举个例子，假设我们用用户的点击行为当做语料，当我们拿到一个用户的历史点击行为时，比如是list(商品A, 商品B，商品C，商品D)，很有可能商品B是用户搜索了连衣裙后点的最后一个商品，而商品C是用户搜索了手机后点击的商品，如果我们不做区分，模型会认为B和C处以一个上下文。
具体的session定义要根据自身的业务诉求来，不存在标准答案，比如上面的例子，如果你要做用户跨兴趣点的变换表达，也是可以的，论文中给出了airbnb的规则：
A new session is started whenever there is a time gap of more than 30 minutes between two consecutive user clicks.
值得一提的是，论文中用点击行为代表短期兴趣和booking行为代表长期兴趣，分别构建Embedding向量。关于长短期兴趣，业界讨论很多，我的理解是长期兴趣更稳定，但直接用单个用户行为太稀疏了，无法直接训练，一般会先对用户做聚类再训练。
如何让Embedding向量学到东西 模型细节 一般情况下，我们直接用Word2vec，效果就挺好。论文作者根据Airbnb的业务特点，做了点改造，主要集中在目标函数的细节上，比较出彩。先来看一张图： 主要idea是增加一个global context，普通的word2vec在训练过程中，词的context是随着窗口滑动而变化，这个global context是不变的，原文描述如下：
Both are useful from the standpoint of capturing contextual similarity, however booked sessions can be used to adapt the optimization such that at each step we predict not only the neighboring clicked listings but the eventually booked listing as well....</p></div><footer class=entry-footer><span title='2022-10-06 15:33:57 +0800 +0800'>October 6, 2022</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to 不一样的论文解读：2018 Real Time Personalization Using Embeddings for Search Ranking at Airbnb" href=https://whbzju.github.io/posts/2018-real-time-personalization-using-embeddings-for-search-ranking-at-airbnb/></a></article><article class=post-entry><header class=entry-header><h2>Migrated Blog From Octopress to Hugo</h2></header><div class=entry-content><p>自从社交媒体火起来后，很少用在blog写文章，这几年在知乎专栏写的比较多，收获了不少朋友，这个blog荒废多年。但鉴于社媒氛围的剧烈变化，还是blog这个老地方安静，后续会重启，作为一个自留地使用。
把域名买了后发现octopress已经是10年+前的事情，且早已经不维护, 准备迁移到hugo上，花了时间没有找到完美的迁移方式，多多少少会有些格式问题，后面慢慢修吧。好在这个blog的主要内容都是早期工作时间写的，内容质量一般，但也是自己成长的一部分，也懒得删掉了，对阅读起来体验不好的朋友道个歉。</p></div><footer class=entry-footer><span title='2022-10-06 15:07:40 +0800 +0800'>October 6, 2022</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to Migrated Blog From Octopress to Hugo" href=https://whbzju.github.io/posts/migrated-blog-from-octopress-to-hugo/></a></article><article class=post-entry><header class=entry-header><h2>微店引发的互联网模式思考</h2></header><div class=entry-content><p>现有爆发的互联网模式，体现在将原先只有一小部分人能享受到的服务面向大众，从而实现对现有产业的改造。比如小米，原先苹果三星的价格导致大部分用户无法享受他们的产品。小米提供了一个相对优质的产品，牺牲了不少品质，比如外观、做工，但成功将价格定在大众可接受的区间，从而实现了爆发式的增长。这种例子，传统行业也有过很多例子，比如优衣库、Zara，将上流用户的衣服，通过降低一定的质量和设计，完成价格革命，让普通用户享受到时尚，从而实现服装行业的革命。相同的例子还有福特，将汽车面向普通大众。这些例子说明一个，在品质和价格间做好balance，将优质的服务面向大众，具有极大的潜力，同时也说明了一个行业趋向成熟。
回过头来仔细想下，为何苹果三星不直接面向大众提供服务。这里面除了品牌定位，还涉及到一个行业的发展。在行业发展初期，各种成本都是高昂的，所以只能面向一些高端用户。高端用户的定义可以是：对价格不敏感，但对品质有较高的要求。够用和追求品质是区分普通商品和奢侈品的重要区别，俗话说一分钱一分货，十分钱两分货。奢侈品对于人类有重要的意义，在这些no functional的地方持续的投入，恰是文明的来源。拉回来看前面的例子，若要将高端用户使用的服务面向大众，其品质必然会有所下降，但会进入一个处于可接受的范围，接着极力的压榨价格区间，进入市场后照成破坏性的增长，一家独霸。这里小米做到了，淘宝也做到了一半。
来看今天的主题—微店，即微商。上面说到淘宝做到了一半，即对传统的零售商业模式的冲击并没有完成一家独霸，彻底改造。其原因在于，其商品价格下来后，优质的服务不能保证，即没有进入一个合理的区间。这里面的问题比较复杂，刨去假货不说，很多时候商品质量不好，真就是商家无法提供优质的服务。因为这些优质的服务被传统大佬们垄断着。淘宝可以抹除店面、分销的成本，但无法解决生产的问题，当这些大佬进入电商后，很快就被击垮。那为什么要提到微商，因为中国作为一个世界大工厂已经很多年，其生产能力在不断的提升，但缺少产品设计和分销渠道。而微商类似于，拥有这些资源的个人，不愿意被传统的渠道垄断，但有无法承担建立品牌的费用，借力与人与人直接的信任传播来实现分销。
这里重点讲下品牌，淘宝最大的问题还是在于流量越来越贵，小商户直接就出局了，没有办法顺利的进入成长期。而传统的大佬在建立品牌投入的巨大成本，需要靠一定的利润空间收回。所以，如果想打赢这场仗，必须抹除建立品牌的成本，而这也是微商可能能够提供的。天下没有免费得晚餐，如果不是某个环节的效率提高了导致价格的下降，这种模式是不可持续的。这也是为什么目前微商中的代购、面膜之类的东西不能长远的原因。从这点来看当前几个比较热的互联网公司，能得到一定的启示。拨开热闹的表象，观察其是否能照成流程或生产力的提高。比如小米最近的装修，就是一个极好的商业切入点，非常值得资本进入烧钱。同样的还有快的和滴滴。
最后，互联网模式有个重要的特点，即scalable—即高度可复制性，这点下次再讨论。</p></div><footer class=entry-footer><span title='2015-04-18 20:48:00 +0800 +0800'>April 18, 2015</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 微店引发的互联网模式思考" href=https://whbzju.github.io/posts/think-about-weidian-bussiness/></a></article><article class=post-entry><header class=entry-header><h2>Kaggle入门总结</h2></header><div class=entry-content><p>在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period....</p></div><footer class=entry-footer><span title='2015-04-18 20:38:00 +0800 +0800'>April 18, 2015</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to Kaggle入门总结" href=https://whbzju.github.io/posts/kaggle-bike/></a></article><article class=post-entry><header class=entry-header><h2>random forest</h2></header><div class=entry-content><p>概述 知识背景要求 本文要求读者对机器学习中的一些基本概念有一定了解，比如特征，交叉验证，generation等概念。随机森林基于决策树模型，读者事先最好对决策树有一定的了解，若完全不了解，请将文中的tree抽象成能告诉你对错的一个black box，则不会影响理解。
目录 基本思想 理论保证 实践中常用的特性 实践效果验证 需要重点注意的 参考 基本思想 Ensemble method ensemble是当前主流机器学习领域一个非常流行的概念。引用sklearn的文档：
The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.
其又分为两大类：averaging和boosting，分别以Random Forest和AdaBoost算法为代表。
Random Forest 引用wiki的定义：
Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees....</p></div><footer class=entry-footer><span title='2015-02-16 15:09:00 +0800 +0800'>February 16, 2015</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to random forest" href=https://whbzju.github.io/posts/random-forest/></a></article><article class=post-entry><header class=entry-header><h2>机器学习技巧之feature_hashing</h2></header><div class=entry-content><p>问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。
再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。 维基上的定义：
In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array...</p></div><footer class=entry-footer><span title='2015-02-10 15:58:00 +0800 +0800'>February 10, 2015</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to 机器学习技巧之feature_hashing" href=https://whbzju.github.io/posts/feature-hashing/></a></article><article class=post-entry><header class=entry-header><h2>git 实践</h2></header><div class=entry-content><p>git rebase和git branch svn过来的同学一定会觉得git的分支管理好方便，但更应该了解的是git rebase。可以说，用不用git rebase是区分你熟不熟悉git的重要方式。
##git reset
git fetch和git pull git commit -amend</p></div><footer class=entry-footer><span title='2015-02-07 08:54:00 +0800 +0800'>February 7, 2015</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to git 实践" href=https://whbzju.github.io/posts/git-practices/></a></article><article class=post-entry><header class=entry-header><h2>2014读过的书和参加的公开课</h2></header><div class=entry-content><p>技术类 机器学习 Pattern Recognizition and Machine Learning. 目前完成前四章，还没有完全吸收，希望2015的成长能够顺利吸收这本书的知识。 推荐系统cookbook。感觉这本书有些落后于时代，大致翻了下。 推荐系统实践–项亮。入门好读物。 Frontiers in Massive Data Analysis。综述型，推荐。 The Elements of Statistical Learning : Data Mining, Inference, andPrediction。当做工具书，还没有读多少，先把PRML搞定。 kaggle solution分享。 Python python cookbook. 非常推荐。 廖雪峰的python教程. 简洁概要，实战内容有难度，很适合提供自己编程水平。 python for data analysis。了解到pandas，数据分析利器。 sklearn官方文档。图文并茂，极力推荐。 pandas官方文档。例子丰富，入门先推荐10min那篇。 R 粗略看了写文档。
非技术类 文明之光（上下），推荐，吴军博士的书质量一如既往。 女士品茶。概率论发展史及大牛八卦。 英语语法俱乐部–施元佑。大力推荐，介绍语法的来龙去脉。以前太不看重语法，阅读和写作的瓶颈。 金字塔原理。 思考的艺术 如何阅读一本书 数理统计简史 公开课 Andrew Ng的Machine Learing。完成 台大的机器学习基石。完成 Functional Programming Principles in Scala。继续上 Mining Massive Datasets，正在上 機器學習技法 (Machine Learning Techniques)。正在上 互联网资讯 咨询：依旧是知乎，目前没有看到什么能替代。 kaggle，非常好的机器学习学习平台 cousera，2015能学更多有价值的课程。</p></div><footer class=entry-footer><span title='2015-01-02 10:45:00 +0800 +0800'>January 2, 2015</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 2014读过的书和参加的公开课" href=https://whbzju.github.io/posts/2014-readlist/></a></article><article class=post-entry><header class=entry-header><h2>机器学习基石课程总结</h2></header><div class=entry-content><p>课程一开始，提了四个topic，what every machine learning user should know
* when can ml learn * why can ml learn * how can ml learn * how can ml learn better When can ml learn 首先，机器学习针对的场景，通过A对D和H学习一个g，用来描述最终的目标f，而这个事情无法简单的用规则搞定。其次，澄清各类细分ml场景的定义：
* 监督式 * 非监督式 * 增强学习 * 推进系统 * Activity学习，通过asking来学习 * Streaming why can ml learn * shatter的概念 * break point的概念 * generation问题 * VC维的概念 how can ml learn 讲了一些基本的linear方法，比如logistic regression，顺便提了下nonlinear的问题，通过transform将nonlinear映射到linear可分的空间，有点类似核函数，需要进一步确认。
how can ml learn better * overfiting * regularition，这块数学不错。从拉格朗日的constraint说起，到L1和L2的直观意义。 * cv * 三个重要的Principle。Occam's Razor， Sample Bias， Data Snooping....</p></div><footer class=entry-footer><span title='2014-12-28 11:07:00 +0800 +0800'>December 28, 2014</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 机器学习基石课程总结" href=https://whbzju.github.io/posts/about-ml-fundation-course/></a></article><article class=post-entry><header class=entry-header><h2>记第七届R语言大会</h2></header><div class=entry-content><p>这届R语言大会在杭师仓前校区举行，由阿里巴巴承办。邀请到了libsvm的作者林老师。但据同事说林教授演讲用的ppt都是一套（汗~~~）。上午是主会场，有四位嘉宾做了介绍，其他几位介绍的比较范，林稍微带点干货，特别提到数据没有到20T，不一定要上big data。下午是分会场，本人去听了：
分析师使用的R包 京东的ctr模型 天猫learning to rank 天猫数据驱动运营。 点评 京东分享推荐的ctr模型。和我街处于同一起跑线，模型和特征做法都很相似。赞下京东的分享着，讲的很实在。 天猫l2r。没什么干货，介绍了一些基本概念就结束了。 天猫数据驱动运营。预测销量，c2b。介绍的比较范，而且个人觉得他讲的效果有夸大的嫌疑。 分析师用的R包。R中类似ipython notebook的东西。可交互的图，非常实用。 感想 Spark出现的频率很高。 用R来作云服务，我个人觉得不靠谱。 libsvm的作者好有趣。演讲前看论文，演讲后睡觉。工业界范十足。 学术界的分享依旧不靠谱。 IBM关于电信领域的挖掘是来搞笑的吗 接下来就等ppt了。 最后，和阿里比，其他公司的所处的阶段还是非常的初级。</p></div><footer class=entry-footer><span title='2014-11-30 11:01:00 +0800 +0800'>November 30, 2014</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 记第七届R语言大会" href=https://whbzju.github.io/posts/r-language-conference/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://whbzju.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://whbzju.github.io/>Wujia's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>