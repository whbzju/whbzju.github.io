<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>机器学习技巧之feature_hashing | Wujia Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。
再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。 维基上的定义：
In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array"><meta name=generator content="Hugo 0.104.3"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="机器学习技巧之feature_hashing"><meta property="og:description" content="问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。
再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。 维基上的定义：
In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array"><meta property="og:type" content="article"><meta property="og:url" content="https://whbzju.github.io/posts/feature-hashing/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2015-02-10T15:58:00+08:00"><meta property="article:modified_time" content="2015-02-10T15:58:00+08:00"><meta itemprop=name content="机器学习技巧之feature_hashing"><meta itemprop=description content="问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。
再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。 维基上的定义：
In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array"><meta itemprop=datePublished content="2015-02-10T15:58:00+08:00"><meta itemprop=dateModified content="2015-02-10T15:58:00+08:00"><meta itemprop=wordCount content="338"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习技巧之feature_hashing"><meta name=twitter:description content="问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。
再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。 维基上的定义：
In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array"></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Wujia Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">机器学习技巧之feature_hashing</h1><time class="f6 mv4 dib tracked" datetime=2015-02-10T15:58:00+08:00>February 10, 2015</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id=问题>问题</h2><p>最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3>1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。</p><p>再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。
维基上的定义：</p><blockquote><p>In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array</p></blockquote><h2 id=解决方案>解决方案</h2><p>维基上关于Feature Hash的描述非常清晰，各位自己去看，不再累赘，这里多说一点hash的方法。常见的有以下两种实现：</p><pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     x[h mod N] += 1
 return x
</code></pre><p>另外还有一种：</p><pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     idx := h mod N
     if ξ(f) == 1:
         x[idx] += 1
     else:
         x[idx] -= 1
 return x
</code></pre><p>可以理解，既然是hash，必然要付出collision时的代价。实现方案一并没有考虑处理冲突，N越长，冲突的概率越低，然后存储的要求会变大。实现二，通过有符号的hash来解决冲突问题，即有很大概率在出现冲突时，该hash值为0，即不起作用，更详细的描述参考文献2.</p><h2 id=sklearn-featurehasher的实现>sklearn FeatureHasher的实现</h2><p><code>class sklearn.feature_extraction.FeatureHasher(n_features=1048576, input_type='dict', dtype=&lt;type 'numpy.float64'>, non_negative=False)</code>，该接口返回一个sparse类型的array。</p><blockquote><p>The hash function employed is the signed 32-bit version of Murmurhash3.</p></blockquote><p>该接口需要注意的是数据入参，支持三种格式：pair、dict和string。可以参考官方的<a href=https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/tests/test_feature_hasher.py>featurehasher test</a>。</p><p>stackoverflow上也有一个比较好的例子：</p><p>Q:</p><pre tabindex=0><code>I am using FeatureHasher in scikit-learn.

Can anyone explain why I end up with 4 non zero data in the sparse matrix instead of 2 after the following:

&gt;&gt;&gt; f = FeatureHasher(input_type=&#39;string&#39;)
&gt;&gt;&gt; g = f.transform((&#39;as&#39;,&#39;bs&#39;))
&lt;2x1048576 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;
with 4 stored elements in Compressed Sparse Row format&gt;
&gt;&gt;&gt; g=f.transform((&#39;as&#39;,&#39;bs&#39;))
&gt;&gt;&gt; g.data
array([-1.,  1., -1., -1.])
&gt;&gt;&gt; g.nonzero()
(array([0, 0, 1, 1], dtype=int32), array([341263, 354738,  98813, 341263], dtype=int32))
</code></pre><p>A:</p><pre tabindex=0><code>It appears is expecting a sequence of sequences. The outer sequence being for the observations, and the inner being features. With your input, the inner sequence are the characters of the string.

Observation 0: &#39;a&#39; -&gt; 354738, &#39;s&#39; -&gt; 341263

Observation 1: &#39;b&#39; -&gt; 98813, &#39;s&#39; -&gt; 341263

Try this:

g = f.transform([[&#39;as&#39;],[&#39;bs&#39;]])
For output:

&gt;&gt;&gt; g.nonzero()
(array([0, 1], dtype=int32), array([494108, 335425], dtype=int32))
</code></pre><h2 id=参考文献>参考文献</h2><p>[1] <a href=http://en.wikipedia.org/wiki/Feature_hashing>Feature hashing From Wikipedia, the free encyclopedia</a></p><p>[2] <a href=http://alex.smola.org/papers/2009/Weinbergeretal09.pdf>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.</a></p><p>[3] <a href=http://scikit-learn.org/stable/modules/feature_extraction.html>sklearn feature hashing</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">What's in this posts</p><nav id=TableOfContents><ul><li><a href=#问题>问题</a></li><li><a href=#解决方案>解决方案</a></li><li><a href=#sklearn-featurehasher的实现>sklearn FeatureHasher的实现</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://whbzju.github.io/>&copy; Wujia Blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>