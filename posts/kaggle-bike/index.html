<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kaggle入门总结 | Wujia's Blog</title><meta name=keywords content="machine,learning"><meta name=description content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><meta name=author content><link rel=canonical href=https://whbzju.github.io/posts/kaggle-bike/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://whbzju.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://whbzju.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://whbzju.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://whbzju.github.io/apple-touch-icon.png><link rel=mask-icon href=https://whbzju.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Kaggle入门总结"><meta property="og:description" content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><meta property="og:type" content="article"><meta property="og:url" content="https://whbzju.github.io/posts/kaggle-bike/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2015-04-18T20:38:00+08:00"><meta property="article:modified_time" content="2015-04-18T20:38:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kaggle入门总结"><meta name=twitter:description content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://whbzju.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Kaggle入门总结","item":"https://whbzju.github.io/posts/kaggle-bike/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kaggle入门总结","name":"Kaggle入门总结","description":"在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。\n工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：\nipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。\nipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.\n本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.","keywords":["machine","learning"],"articleBody":"在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。\n工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：\nipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。\nipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.\n本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n该比赛的好处是符合kaggle的特点。\nfeature engineering是第一要素。 ensemble大法好。bagging、averaging一起上，比如到最后我喜欢用random forest + gbdt组合再搞一把，总能给我惊喜。 时刻小心过拟合。 特征工程 异常值处理，nan，outlier等。hist和box是两个常用的图形工具。 数据分布倾斜。 log变化、正负样本重新抽样等。 特征交叉组合 pca或random forest的特征重要性选择。 特征之间的相关系数。 特征onehotencoding 模型选择 Logistic regression Random Forest gbdt和gbrt Factorization Machines LR对特征要求更高，比如很多categorical的特征要作binary编码。我个人倾斜于RF和gbdt，都是属于ensemble思想下的算法。具体RF算法可以参考以前写的一篇blog：random forest\nFactorization Machines这个算法好多人用它做ctr预估，后续可以研究下。\n代码分享 具体代码实现请看我的notebook： kaggle上的自行车出租数量预测\n参考文献 [1] 知乎答案Kaggle如何入门\n[2] 知乎答案参加kaggle竞赛是怎样一种体验？\n","wordCount":"123","inLanguage":"en","datePublished":"2015-04-18T20:38:00+08:00","dateModified":"2015-04-18T20:38:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://whbzju.github.io/posts/kaggle-bike/"},"publisher":{"@type":"Organization","name":"Wujia's Blog","logo":{"@type":"ImageObject","url":"https://whbzju.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://whbzju.github.io/ accesskey=h title="Wujia's Blog (Alt + H)">Wujia's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Kaggle入门总结</h1><div class=post-meta><span title='2015-04-18 20:38:00 +0800 +0800'>April 18, 2015</span></div></header><div class=post-content><p>在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。</p><h2 id=工具>工具<a hidden class=anchor aria-hidden=true href=#工具>#</a></h2><p>有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：</p><pre><code>ipython notebook + pandas + sklearn
</code></pre><p>在面对特别大的数据集，使用了公司的spark。</p><ul><li>ipython notebook，神器，请参考我的另一篇blog <a href=http://www.wujiame.com/blog/2014/11/23/ipython-notebook-bring-to-me/>Ipython Notebook对机器学习工程师的价值</a></li><li>pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。</li><li>sklearn：在github上非常活跃的项目，请多读官方文档。</li><li>spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。</li></ul><h2 id=比赛选择>比赛选择<a hidden class=anchor aria-hidden=true href=#比赛选择>#</a></h2><p>首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法<a href=http://www.wujiame.com/blog/2015/02/10/feature-hashing/>featrue hashing</a>.</p><p>本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：</p><blockquote><p>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.</p></blockquote><p>该比赛的好处是符合kaggle的特点。</p><ol><li>feature engineering是第一要素。</li><li>ensemble大法好。bagging、averaging一起上，比如到最后我喜欢用random forest + gbdt组合再搞一把，总能给我惊喜。</li><li>时刻小心过拟合。</li></ol><h2 id=特征工程>特征工程<a hidden class=anchor aria-hidden=true href=#特征工程>#</a></h2><ol><li>异常值处理，nan，outlier等。hist和box是两个常用的图形工具。</li><li>数据分布倾斜。 log变化、正负样本重新抽样等。</li><li>特征交叉组合</li><li>pca或random forest的特征重要性选择。</li><li>特征之间的相关系数。</li><li>特征onehotencoding</li></ol><h2 id=模型选择>模型选择<a hidden class=anchor aria-hidden=true href=#模型选择>#</a></h2><ol><li>Logistic regression</li><li>Random Forest</li><li>gbdt和gbrt</li><li>Factorization Machines</li></ol><p>LR对特征要求更高，比如很多categorical的特征要作binary编码。我个人倾斜于RF和gbdt，都是属于ensemble思想下的算法。具体RF算法可以参考以前写的一篇blog：<a href=http://www.wujiame.com/blog/2015/02/16/random-forest/>random forest</a></p><p>Factorization Machines这个算法好多人用它做ctr预估，后续可以研究下。</p><h2 id=代码分享>代码分享<a hidden class=anchor aria-hidden=true href=#代码分享>#</a></h2><p>具体代码实现请看我的notebook：
<a href=http://nbviewer.ipython.org/gist/whbzju/ff06fce9fd738dcf8096>kaggle上的自行车出租数量预测</a></p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] 知乎答案<a href=http://www.zhihu.com/question/23987009>Kaggle如何入门</a></p><p>[2] 知乎答案<a href=http://www.zhihu.com/question/24533374>参加kaggle竞赛是怎样一种体验？</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://whbzju.github.io/tags/machine/>machine</a></li><li><a href=https://whbzju.github.io/tags/learning/>learning</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://whbzju.github.io/>Wujia's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>