<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Kaggle入门总结 | Wujia Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><meta name=generator content="Hugo 0.104.3"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Kaggle入门总结"><meta property="og:description" content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><meta property="og:type" content="article"><meta property="og:url" content="https://whbzju.github.io/posts/kaggle-bike/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2015-04-18T20:38:00+08:00"><meta property="article:modified_time" content="2015-04-18T20:38:00+08:00"><meta itemprop=name content="Kaggle入门总结"><meta itemprop=description content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."><meta itemprop=datePublished content="2015-04-18T20:38:00+08:00"><meta itemprop=dateModified content="2015-04-18T20:38:00+08:00"><meta itemprop=wordCount content="123"><meta itemprop=keywords content="machine,learning,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kaggle入门总结"><meta name=twitter:description content="在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Wujia Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Kaggle入门总结</h1><time class="f6 mv4 dib tracked" datetime=2015-04-18T20:38:00+08:00>April 18, 2015</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。</p><h2 id=工具>工具</h2><p>有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：</p><pre><code>ipython notebook + pandas + sklearn
</code></pre><p>在面对特别大的数据集，使用了公司的spark。</p><ul><li>ipython notebook，神器，请参考我的另一篇blog <a href=http://www.wujiame.com/blog/2014/11/23/ipython-notebook-bring-to-me/>Ipython Notebook对机器学习工程师的价值</a></li><li>pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。</li><li>sklearn：在github上非常活跃的项目，请多读官方文档。</li><li>spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。</li></ul><h2 id=比赛选择>比赛选择</h2><p>首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法<a href=http://www.wujiame.com/blog/2015/02/10/feature-hashing/>featrue hashing</a>.</p><p>本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：</p><blockquote><p>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.</p></blockquote><p>该比赛的好处是符合kaggle的特点。</p><ol><li>feature engineering是第一要素。</li><li>ensemble大法好。bagging、averaging一起上，比如到最后我喜欢用random forest + gbdt组合再搞一把，总能给我惊喜。</li><li>时刻小心过拟合。</li></ol><h2 id=特征工程>特征工程</h2><ol><li>异常值处理，nan，outlier等。hist和box是两个常用的图形工具。</li><li>数据分布倾斜。 log变化、正负样本重新抽样等。</li><li>特征交叉组合</li><li>pca或random forest的特征重要性选择。</li><li>特征之间的相关系数。</li><li>特征onehotencoding</li></ol><h2 id=模型选择>模型选择</h2><ol><li>Logistic regression</li><li>Random Forest</li><li>gbdt和gbrt</li><li>Factorization Machines</li></ol><p>LR对特征要求更高，比如很多categorical的特征要作binary编码。我个人倾斜于RF和gbdt，都是属于ensemble思想下的算法。具体RF算法可以参考以前写的一篇blog：<a href=http://www.wujiame.com/blog/2015/02/16/random-forest/>random forest</a></p><p>Factorization Machines这个算法好多人用它做ctr预估，后续可以研究下。</p><h2 id=代码分享>代码分享</h2><p>具体代码实现请看我的notebook：
<a href=http://nbviewer.ipython.org/gist/whbzju/ff06fce9fd738dcf8096>kaggle上的自行车出租数量预测</a></p><h2 id=参考文献>参考文献</h2><p>[1] 知乎答案<a href=http://www.zhihu.com/question/23987009>Kaggle如何入门</a></p><p>[2] 知乎答案<a href=http://www.zhihu.com/question/24533374>参加kaggle竞赛是怎样一种体验？</a></p><ul class=pa0><li class="list di"><a href=/tags/machine/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">machine</a></li><li class="list di"><a href=/tags/learning/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">learning</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">What's in this posts</p><nav id=TableOfContents><ul><li><a href=#工具>工具</a></li><li><a href=#比赛选择>比赛选择</a></li><li><a href=#特征工程>特征工程</a></li><li><a href=#模型选择>模型选择</a></li><li><a href=#代码分享>代码分享</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">Related</p><ul class="pa0 list"><li class=mb2><a href=/posts/random-forest/>random forest</a></li><li class=mb2><a href=/posts/about-ml-fundation-course/>机器学习基石课程总结</a></li><li class=mb2><a href=/posts/ipython-notebook-bring-to-me/>ipython notebook对机器学习工程师的价值</a></li><li class=mb2><a href=/posts/ml-distance-measure/>聚类算法中常见的距离计算方法</a></li><li class=mb2><a href=/posts/r-language-conference/>记第七届R语言大会</a></li></ul></div></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://whbzju.github.io/>&copy; Wujia Blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>