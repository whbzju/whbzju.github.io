<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>machine on Wujia's Blog</title><link>https://www.wujia.io/tags/machine/</link><description>Recent content in machine on Wujia's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 18 Apr 2015 20:38:00 +0800</lastBuildDate><atom:link href="https://www.wujia.io/tags/machine/index.xml" rel="self" type="application/rss+xml"/><item><title>Kaggle入门总结</title><link>https://www.wujia.io/posts/kaggle-bike/</link><pubDate>Sat, 18 Apr 2015 20:38:00 +0800</pubDate><guid>https://www.wujia.io/posts/kaggle-bike/</guid><description>在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。
工具 有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：
ipython notebook + pandas + sklearn 在面对特别大的数据集，使用了公司的spark。
ipython notebook，神器，请参考我的另一篇blog Ipython Notebook对机器学习工程师的价值 pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。 sklearn：在github上非常活跃的项目，请多读官方文档。 spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。 比赛选择 首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法featrue hashing.
本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.</description></item><item><title>random forest</title><link>https://www.wujia.io/posts/random-forest/</link><pubDate>Mon, 16 Feb 2015 15:09:00 +0800</pubDate><guid>https://www.wujia.io/posts/random-forest/</guid><description>概述 知识背景要求 本文要求读者对机器学习中的一些基本概念有一定了解，比如特征，交叉验证，generation等概念。随机森林基于决策树模型，读者事先最好对决策树有一定的了解，若完全不了解，请将文中的tree抽象成能告诉你对错的一个black box，则不会影响理解。
目录 基本思想 理论保证 实践中常用的特性 实践效果验证 需要重点注意的 参考 基本思想 Ensemble method ensemble是当前主流机器学习领域一个非常流行的概念。引用sklearn的文档：
The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.
其又分为两大类：averaging和boosting，分别以Random Forest和AdaBoost算法为代表。
Random Forest 引用wiki的定义：
Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.</description></item><item><title>机器学习基石课程总结</title><link>https://www.wujia.io/posts/about-ml-fundation-course/</link><pubDate>Sun, 28 Dec 2014 11:07:00 +0800</pubDate><guid>https://www.wujia.io/posts/about-ml-fundation-course/</guid><description>课程一开始，提了四个topic，what every machine learning user should know
* when can ml learn * why can ml learn * how can ml learn * how can ml learn better When can ml learn 首先，机器学习针对的场景，通过A对D和H学习一个g，用来描述最终的目标f，而这个事情无法简单的用规则搞定。其次，澄清各类细分ml场景的定义：
* 监督式 * 非监督式 * 增强学习 * 推进系统 * Activity学习，通过asking来学习 * Streaming why can ml learn * shatter的概念 * break point的概念 * generation问题 * VC维的概念 how can ml learn 讲了一些基本的linear方法，比如logistic regression，顺便提了下nonlinear的问题，通过transform将nonlinear映射到linear可分的空间，有点类似核函数，需要进一步确认。
how can ml learn better * overfiting * regularition，这块数学不错。从拉格朗日的constraint说起，到L1和L2的直观意义。 * cv * 三个重要的Principle。Occam's Razor， Sample Bias， Data Snooping.</description></item><item><title>ipython notebook对机器学习工程师的价值</title><link>https://www.wujia.io/posts/ipython-notebook-bring-to-me/</link><pubDate>Sun, 23 Nov 2014 16:17:00 +0800</pubDate><guid>https://www.wujia.io/posts/ipython-notebook-bring-to-me/</guid><description>关键词：代码、数据、文档合一。
&amp;mdash;-draft: false toc: true 如果选一个关键词来描述机器学习工程师的工作，不断试错是我心中的number one。相对于软件工程师来讲，有大量琐碎的dirty需要做，通常会占据到80%左右的时间。一个好的工具能够极大的提高效率。
总结需求如下：
可交互式的环境：比如预处理数据，有的时候数据比较大，比较耗时，希望能处理一次后就放在内存里面使用。 文档化，记录工作流。数据挖掘会有非常多的idea要去尝试，实现这些idea的代码会有微小的差异，需要一个工具能够统一追踪管理他们。且不同的实验会有不同的结果，整理这些结果形成文档太费时间，希望能够做完实验就生成文档。 经常会有一些片段代码要写，写在文件里有太零碎，写在交互式的shell里面有很难回溯，需要一个交互式和文档结合的工具。 支持可视化工具，兼容python画图 神奇的ipython notebook 安装环境 非常简单，推荐：Anaconda, 官网
ipython notebook入门 还是官网, 一开始不适应的同学，多看几个example吧。
分享你的ipython notebook 一键分享：A simple way to share Jupyter Notebooks
最后 附一张我的在kaggle上用ipython notebook做的一个入门题照：</description></item><item><title>聚类算法中常见的距离计算方法</title><link>https://www.wujia.io/posts/ml-distance-measure/</link><pubDate>Sat, 22 Nov 2014 22:01:00 +0800</pubDate><guid>https://www.wujia.io/posts/ml-distance-measure/</guid><description>概述 在面对聚类问题时，选择何种距离计算方法求相似度是一个basic question。文献[1]中提到了N多计算方法，从大类来看有以下几种：
$L_p$ Minkowski家族 $ L_1 $ 家族 Intersection 家族 Inner Product家族 etl 简单算一下大概有40+个计算方法，其中有好多没有听过。好在工业界一般只涉及到几个，本文将按自己理解大致介绍下这些方法及应用情况。 距离的类型和尺度 类型：
二进制（binary） 离散值（Discrete） 连续值(Continuous) 尺度：
定性：比如同义：red、green、black，比如顺序：高、中、低 定量： a) interval b) ratio 距离的类型和尺度非常重要，影响后续聚类算法的选择。 距离计算方法定义 严谨的定义参考[4]，通俗来讲，在一个空间内，距离计算方法满足以下4个公理。
$d(x,y) ≥ 0$ $ d(x,y)=0$ if $x=y$ $ d(x, y) = d(y, x)$ (distance is symmetric) $d(x, y) ≤ d(x, z) + d(z, y)$ (the triangle inequality). 在欧式空间，第四个公理可以直观理解为两点之间距离最短。在其他情况需要一些证明才能推导。
常见的距离计算方法 Lr norm 在n维空间，其计算公式如下：
$d(x,y)=(\sum_{k=1}^{n} |x_k-y_k| ^r)^{1/r}$
欧式距离 当r=2，这就是我们熟悉的欧式距离，其聚类形状在二维空间是一个圆。归属于$L_2$ norm
曼哈顿距离 r=1，归属于$L_1$ norm，其名字的来源与该距离计算过程有关。该距离类似在x和y的每个维度上沿grid line上travel，类似曼哈顿的街道。</description></item></channel></rss>